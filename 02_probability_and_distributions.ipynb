{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 2: Uncertainty\n",
    "\n",
    "## Associated Reading: Bishop 1.2\n",
    "\n",
    "# 1. Overfitting\n",
    "This week, we'll continue our analysis of the simple infant mortality rate that we have been working with.  Once again, we will read the data from file, drop outliers, and normalize.  However, this time, let's imagine that rather than just predicting the IMR for Venezuela, we'll try to make predictions for 1/2 of the countries on the list.  Thus, we're going to split our data into two sets: a *training set*, which we will use to adjust the parameters of (or train) our model, and a *test set*, which we will use after training to determine how well our model is at predicting things it hasn't seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(17)\n",
    "\n",
    "data = pd.read_csv('datasets/birthrate.dat',header=0,sep=r\"\\s{2,}\",engine='python',index_col=0)\n",
    "data = data.drop('United States')\n",
    "\n",
    "pci_min = data['PCI'].min()\n",
    "pci_range = data['PCI'].max() - pci_min\n",
    "x = (data['PCI'] - pci_min)/pci_range\n",
    "\n",
    "imr_min = data['Infant Mortality'].min()\n",
    "imr_range = data['Infant Mortality'].max() - imr_min\n",
    "y = (data['Infant Mortality'] - imr_min)/imr_range\n",
    "\n",
    "m = len(x)\n",
    "random_indices = np.random.permutation(range(m))\n",
    "train_indices = random_indices[:m//2]\n",
    "test_indices = random_indices[m//2:]\n",
    "\n",
    "x_train = x[train_indices]\n",
    "y_train = y[train_indices]\n",
    "\n",
    "x_test = x[test_indices]\n",
    "y_test = y[test_indices]\n",
    "\n",
    "plt.plot(x_train,y_train,'ro')\n",
    "plt.plot(x_test,y_test,'bo')\n",
    "plt.xlabel('Normalized PCI')\n",
    "plt.ylabel('Normalized IMR')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our purpose from the last exercise has changed a bit: rather than predict the value of IMR for a single country, now we want to ask whether, in general, our model is good at predicting *any* country's IMR.  \n",
    "\n",
    "In the last unit's in-class assignment, we developed some code that would allow us to fit polynomials of various complexity to these observations.  We also found that for models with many degrees of freedom (lots of parameters relative to the data), that some of the resulting fits were pretty wild."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_poly(x,y,d):\n",
    "    X = np.vander(x,d,increasing=True)\n",
    "    w = np.linalg.solve(X.T@X,X.T@y)\n",
    "    y_pred = X @ w\n",
    "    return y_pred,w\n",
    "\n",
    "def evaluate_poly(x,w):\n",
    "    X = np.vander(x,len(w),increasing=True)\n",
    "    return X @ w\n",
    "\n",
    "def get_smooth_prediction(w,xmin=0,xmax=1):\n",
    "    x = np.linspace(xmin,xmax,101)\n",
    "    y = evaluate_poly(x,w)\n",
    "    return x,y\n",
    "\n",
    "d = len(x_train)\n",
    "\n",
    "y_pred_train,w = fit_poly(x_train,y_train,d)\n",
    "x_plotting,y_plotting = get_smooth_prediction(w)\n",
    "\n",
    "plt.plot(x_train,y_train,'ro')\n",
    "plt.plot(x_plotting,y_plotting,'k-')\n",
    "plt.xlabel('Normalized PCI')\n",
    "plt.ylabel('Normalized IMR')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we're getting almost a perfect fit to the training data.  **But do we believe the predictions that these complicated models produce?** Let's come up with a strategy to find out.  How shall we quantify how well our model fits the training data?    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mse(y_pred,y_obs):\n",
    "    return 1./len(y_pred)*sum((y_pred-y_obs)**2)\n",
    "\n",
    "mse_train = []\n",
    "w_trained = []\n",
    "\n",
    "for d in range(1,12):\n",
    "    y_train_pred,w = fit_poly(x_train,y_train,d)\n",
    "    mse_train.append(get_mse(y_train_pred,y_train))\n",
    "    w_trained.append(w)\n",
    "    \n",
    "plt.plot(range(0,11),mse_train)\n",
    "plt.xlabel('Polynomial Degree')\n",
    "plt.ylabel('MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is great.  We get what amounts to zero error as the complexity of the model increases. \n",
    "\n",
    "But what about the error when it comes to predicting stuff that we didn't use to train.  We can evaluate that using the *test set* that we split off from the main dataset earlier.  In fact, we can use the same metric to quantify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_test = []\n",
    "\n",
    "for d,w in zip(range(1,12),w_trained):\n",
    "    y_test_pred = evaluate_poly(x_test,w)\n",
    "    mse_test.append(get_mse(y_test_pred,y_test))\n",
    "    \n",
    "plt.plot(range(0,11),mse_test)\n",
    "plt.xlabel('Polynomial Degree')\n",
    "plt.ylabel('MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we plot both the training and test error on top of one another?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(0,11),mse_train,'r-')\n",
    "plt.plot(range(0,11),mse_test,'b-')\n",
    "plt.xlabel('Polynomial Degree')\n",
    "plt.ylabel('MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's training error continues to decline as we add more *complexity* to the model.  However, **based on the test error, is there any advantage to using a more complex model?  at what point does using a more complex model cease to be an advantage?**\n",
    "\n",
    "Perhaps a more critical question is as follows: **What is going wrong when we use a more complex model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Observational uncertainty\n",
    "Taking measurements is hard.  There are many ways in which data may *not perfectly quantify the phenomenon it is supposed to be quantifying*.  Let's take the IMR, for example: there might be cultural biases against reporting or government statistics might be corrupted. This is called *observational uncertainty*.  (More insidiously, it is always the case that the models that we use are incorrect, though some may still be useful.  This latter reason behind why data diverge from predictions is called *structural uncertainty*, however we will ignore this for the time being).  We need our models to be robust to this kind of observational uncertainty: our fitting procedure should, in some sense, be able to ignore these observational errors and still fit the underlying quantity that the observations are imperfectly representing.  Let's experiment with how robust our polynomial models are to observational error.  \n",
    "\n",
    "The code I provide below simulates another reality.  What do I mean by that?  Let's imagine for a moment that the error in our measurements is *random*, in the sense that our observation is just the true value of IMR corrupted by some random number.  What this code does is (for our purposes) to provide the same observations, but with that random number something different.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_y_train(sigma=0.02):\n",
    "    return y_train + sigma*np.random.randn(len(y_train))\n",
    "\n",
    "plt.plot(x_train,get_new_y_train(),'o')\n",
    "plt.plot(x_train,get_new_y_train(),'o')\n",
    "plt.plot(x_train,get_new_y_train(),'o')\n",
    "plt.xlabel('Relative PCI')\n",
    "plt.ylabel('Relative IMR')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the above colors represents a dataset which (very loosely speaking) is the dataset we might have collected in a different reality.  Now we can ask the question, how robust is the process of fitting a line to these errors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = []\n",
    "\n",
    "for i in range(1000):\n",
    "    y_train_new = get_new_y_train(sigma=0.05)\n",
    "    y_train_pred,w = fit_poly(x_train,y_train_new,2)\n",
    "    ws.append(w)\n",
    "    x_plotting,y_plotting = get_smooth_prediction(w)\n",
    "    plt.plot(x_train,y_train_new,'r.',alpha=0.3)\n",
    "    plt.plot(x_plotting,y_plotting,'k-',alpha=0.3)\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
